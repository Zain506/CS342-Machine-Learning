{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "**Zain Nomani 2202455**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X: pd.DataFrame = pd.read_csv(\"data/Train/train_x.csv\", header = None)\n",
    "y: pd.DataFrame = pd.read_csv(\"data/Train/train_y.csv\", header = None)\n",
    "X_test: pd.DataFrame = pd.read_csv(\"data/Test/test_x.csv\", header = None)\n",
    "y_test: pd.DataFrame = pd.read_csv(\"data/Test/test_y.csv\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task A\n",
    "\n",
    "Implement regression tree with stop criterion being \"minimum number of samples required to split a node\" i.e. if node has less samples than a value of min_samples_split then the splitting of the node must stop. Implement from scratch and check performance with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegressionTreeNode class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Node class for regression classifier\n",
    "Represents decision stump\n",
    "Builds classification down to min_samples_split=2\n",
    "Constructs entire tree from current node\n",
    "Testing capabilities included\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class RegressionTreeNode:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - X: pd.DataFrame: Feature vectors assigned to stump\n",
    "        - y: pd.DataFrame: Results of each feature vector assigned to stump\n",
    "        - min_samples_split: int: Hyperparameter used to control complexity\n",
    "        - feature: int: feature used to split data\n",
    "        - threshold: float: point at which data is split on the feature\n",
    "        - samples: int: number of samples assigned to stump\n",
    "        - right_child: Node: Child node assigned X_right and y_right\n",
    "        - left_child: Node: Child node assigned X_left and y_left\n",
    "        - parent: Node: Node for which this self node is either a right_child or left_child\n",
    "    \n",
    "    Public Methods:    \n",
    "        - summarise(): Prints key values of this stump\n",
    "        - train(): Recursively calls _singleRun() to split current and all child nodes\n",
    "        - predict(): Predicts the output of a single feature vector by traversing down tree\n",
    "        - test(): Given multiple feature vectors, calls predict() on each and calculates Error\n",
    "    \n",
    "    Private Methods:\n",
    "        - _singleRun(): NumPy methods to calculate best split point of current node\n",
    "    \n",
    "    Helpful Methods:\n",
    "        - Leaf(): Is the current node a leaf or not?\n",
    "        - Feature(): Returns feature to split on\n",
    "        - Threshold(): Returns threshold value on which data is split\n",
    "        - MSE(): Returns mean squared error of self.y from mean\n",
    "        - Value(): Mean value of self.y on this stump\n",
    "        - __hash__() and __eq__: Hashing for dictionary memoization when recursively running\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.feature = self.threshold = self.right_child = self.left_child = self.parent = None # Feature to be split upon based on X\n",
    "        self.min_samples_split = None\n",
    "        self.leftIndex=None\n",
    "        self.rightIndex=None\n",
    "\n",
    "    \"\"\"Public Methods\"\"\"\n",
    "    \n",
    "    def summarise(self):\n",
    "        print(\"Feature: \", self.Feature())\n",
    "        print(\"Threshold: \", self.Threshold())\n",
    "        print(\"Samples: \", self.Samples())\n",
    "        print(\"Value: \", self.Value())\n",
    "    \n",
    "    def train(self, X: pd.DataFrame, y: pd.DataFrame, min_samples_split: int = None, memo: dict = None) -> \"RegressionTreeNode\":\n",
    "        \"\"\"Recursively call _singleRun()\"\"\"\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(\"Input and output vector mismatch\")\n",
    "        if memo is None:\n",
    "            memo = {}\n",
    "        Xcopy = X.copy()\n",
    "        ycopy = y.copy()\n",
    "        if 'tmp' in Xcopy.columns: # Remove duplicates for ease of calculation\n",
    "            Xcopy = Xcopy.loc[:, X.columns != 'tmp']\n",
    "        Xcopy.insert(len(X.columns), 'tmp', y)\n",
    "        Xcopy = Xcopy.drop_duplicates(keep='first')\n",
    "        ycopy = Xcopy.pop('tmp')\n",
    "        self.X, self.y = Xcopy, ycopy\n",
    "        if min_samples_split is None:\n",
    "            min_samples_split = 2\n",
    "        self.min_samples_split = min_samples_split\n",
    "        if self in memo:\n",
    "            return memo[self]\n",
    "        self = self._singleRun()\n",
    "        memo[self] = self\n",
    "        if self.leftIndex is not None and self.rightIndex is not None:\n",
    "            self.left_child = RegressionTreeNode().train(self.X[self.leftIndex], self.y[self.leftIndex], min_samples_split, memo)\n",
    "            self.right_child = RegressionTreeNode().train(self.X[self.rightIndex], self.y[self.rightIndex], min_samples_split, memo)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, feature_vector: pd.Series, hyperParams: float = None) -> float:\n",
    "        if hyperParams is None:\n",
    "            hyperParams = self.min_samples_split\n",
    "        if hyperParams > self.Samples() or self.Feature() is None:\n",
    "            return self.Value()\n",
    "        elif feature_vector[self.Feature()] <= self.Threshold():\n",
    "            return self.left_child.predict(feature_vector, hyperParams)\n",
    "        elif feature_vector[self.Feature()] > self.Threshold():\n",
    "            return self.right_child.predict(feature_vector, hyperParams)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def test(self, feature_vectors: pd.DataFrame, ground_truth: pd.DataFrame, hyperParams: int = None) -> float:\n",
    "        if hyperParams is None:\n",
    "            hyperParams = self.min_samples_split\n",
    "        if len(feature_vectors) != len(ground_truth):\n",
    "            raise ValueError(f\"There are {len(feature_vectors)} Feature Vectors but {len(ground_truth)} Ground Truths\")\n",
    "        # Output of feature vectors should be NumPy arrays\n",
    "        groundTruth: np.ndarray = ground_truth.to_numpy() # For speed of calculation\n",
    "        predictions: np.ndarray = feature_vectors.apply(lambda x: self.predict(x, hyperParams), axis=1).to_numpy().reshape(-1,1)\n",
    "        return np.nanmean((predictions - groundTruth)**2)\n",
    "    \n",
    "    \"\"\"Private methods\"\"\"\n",
    "    \n",
    "    def _singleRun(self) -> \"RegressionTreeNode\":\n",
    "        \"\"\"Evaluate a single tree to find optimal split point. Create children\"\"\"\n",
    "        if self.Samples() >= self.min_samples_split:\n",
    "            X_vals = self.X.to_numpy().transpose()\n",
    "            y_vals = self.y.to_numpy().transpose()\n",
    "            # Choose 100 values from each feature\n",
    "            # Allows us to create a NumPy array for efficiency\n",
    "            thresholds: np.ndarray = np.linspace(X_vals.min(axis=1), X_vals.max(axis=1), 100).transpose()\n",
    "            result = X_vals[:, np.newaxis, :] <= thresholds[:, :, np.newaxis]\n",
    "            inverse = X_vals[:, np.newaxis, :] > thresholds[:, :, np.newaxis]\n",
    "            # Turn any falses into np.nan to remove them from the multiplication with y_vals\n",
    "            # i.e. Any rows that belong to the other child are turned into np.nan\n",
    "            left = np.where(result.astype(float) == 0, np.nan, result) * y_vals\n",
    "            right = np.where(inverse.astype(float) == 0, np.nan, inverse)*y_vals\n",
    "            mean_left = np.nanmean(left, axis=-1)\n",
    "            mean_right =np.nanmean(right,axis=-1)\n",
    "            mse_left = np.nanmean((left - mean_left[:,:,np.newaxis])**2, axis=-1)\n",
    "            mse_right=np.nanmean((right-mean_right[:, :,np.newaxis])**2, axis=-1)\n",
    "            if len(y_vals) == 0:\n",
    "                return ValueError(\"Length of assigned dataframe is 0\")\n",
    "            factorLeft = np.sum(~np.isnan(left), axis=-1) / len(y_vals)\n",
    "            factorRight = np.sum(~np.isnan(right), axis=-1)/len(y_vals)\n",
    "\n",
    "            loss = factorLeft*mse_left + factorRight*mse_right # Weighted MSE\n",
    "            min = np.nanmin(loss)\n",
    "            argmin = np.where(loss == min)\n",
    "\n",
    "            self.feature = argmin[0][0]\n",
    "            self.threshold = thresholds[argmin[0][0]][argmin[1][0]]\n",
    "\n",
    "            left_indices = result[argmin[0][0]][argmin[1][0]] # Rows of X and y which go to left child\n",
    "            right_indices = ~left_indices\n",
    "            \n",
    "            self.leftIndex = left_indices\n",
    "            self.rightIndex = right_indices\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    \"\"\"Helpful methods\"\"\"\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self.y.index))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, RegressionTreeNode):\n",
    "            return self.y.index == other.y.index\n",
    "        return False\n",
    "    \n",
    "    def Leaf(self) -> bool:\n",
    "        return True if (self.right_child is None and self.left_child is None) else False\n",
    "    \n",
    "    def Feature(self) -> int:\n",
    "        return self.feature\n",
    "    \n",
    "    def Threshold(self) -> float:\n",
    "        return self.threshold\n",
    "\n",
    "    def MSE(self) -> float | None:\n",
    "        return np.mean((self.y.values - np.mean(self.y))**2) if self.samples > 0 else None\n",
    "\n",
    "    def Samples(self) -> int:\n",
    "        if self.y is None or self.X is None:\n",
    "            return None\n",
    "        elif len(self.y) != len(self.X):\n",
    "            return None\n",
    "        else:\n",
    "            return len(self.y)\n",
    "    \n",
    "    def Value(self) -> float:\n",
    "        if len(self.y) == 0:\n",
    "            return None\n",
    "        return self.y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of RegressionTreeNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionTreeNode MSE on training data:  0.0\n",
      "RegressionTreeNode MSE on test data:  0.7092307692307692\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n",
    "tree = RegressionTreeNode().train(X, y, 2)\n",
    "print(\"RegressionTreeNode MSE on training data: \", tree.test(X, y))\n",
    "print(\"RegressionTreeNode MSE on test data: \", tree.test(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing performance vs Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn MSE on training data:  0.0\n",
      "Sklearn MSE on test data:  0.7384615384615385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sklearn_tree = DecisionTreeRegressor()\n",
    "sklearn_tree.fit(X, y)\n",
    "sklearn_test_predictions = sklearn_tree.predict(X_test)\n",
    "sklearn_train_predictions = sklearn_tree.predict(X)\n",
    "mseTrain = mean_squared_error(y, sklearn_train_predictions)\n",
    "mseTest = mean_squared_error(y_test, sklearn_test_predictions)\n",
    "\n",
    "print(\"Sklearn MSE on training data: \", mseTrain)\n",
    "print(\"Sklearn MSE on test data: \", mseTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Task: Using regression tree, fit an ensemble of exactly 5 trees. After training, compute performance on test data, and avoid overfitting by using regularisation techniques to limit complexity of each tree. Use Minimum Samples Split regularisation technique: Hyper-parameter that ensures a node has minimum number of samples before it can be split to prevent tiny splits that create noise. Note: You can set each tree's complexity different to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegressionTreeKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Regression Tree Classifier\n",
    "Builds regression classifier down to min_samples_split=2\n",
    "Can predict using any min_samples_split\n",
    "Wraps around RegressionTreeNode\n",
    "Identifies best min_samples_split using K-fold cross validation\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class RegressionTreeKFold:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - X: pd.DataFrame: Feature vectors as input\n",
    "        - y: pd.DataFrame: Corresponding results of each feature vector\n",
    "        - splits: list[np.ndarray]: List of k lists, each containing indices of samples in each fold\n",
    "        - hyperParams: list[int]: List of hyperparameters to test\n",
    "        - validation: np.ndarray: Initial 3D NumPy array of each fold. Contains boolean indexing for filtering X and y\n",
    "        - errors: np.ndarray: 2D NumPy Array of Mean Square Errors for each fold and hyperparameter\n",
    "        - metrics: np.ndarray: Taking mean of errors: 1D NumPy array of MSE for each hyperParameter\n",
    "        - best: float: Best hyperParameter which minimises the MSE. Found by argmin(metrics)\n",
    "        - root: Node: Root Node of regression tree\n",
    "    \n",
    "    Public Methods:\n",
    "        - train(): Builds regression tree on each fold and calls predict() on each hyperparameter\n",
    "        - predict(): Calls node.predict(vector, hyperParam). Predicts with best value if not specified\n",
    "        - test(): Tests given feature vectors on ground truth and provided hyperParameter - outputs MSE\n",
    "    \n",
    "    Private Methods:\n",
    "        - _makeFolds(): Builds self.splits to create k folds for cross validation\n",
    "        - _getParams(): Constructs list of hyperparameters to test\n",
    "        - _setup(): Build self.validation: 3D array of booleans to split dataframes for training and testing\n",
    "        - _findBest(): Using self.validation, build self.errors by iterating using for loop to test each fold\n",
    "    \"\"\"\n",
    "    def __init__(self): # Initialise all parameters to None\n",
    "        self.X = self.y = self.splits = self.errors = self.metrics = self.best = self.root = None\n",
    "        self.hyperParams: list[int] = None\n",
    "        self.validation = None\n",
    "    \n",
    "    \"\"\"Public Methods\"\"\"\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "        self._makeFolds()\n",
    "        # print(\"Folds created\")\n",
    "        self._getParams()\n",
    "        # print(\"HyperParameters set up\")\n",
    "        self._setup()\n",
    "        # print(\"Matrices set up. Now running...\")\n",
    "        self._findBest()\n",
    "        # print(\"Process complete. Now training final regression tree with param \", self.best)\n",
    "        self.root = RegressionTreeNode().train(self.X, self.y, self.best)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, feature_vector: pd.Series, hyperParams: float = None) -> float:\n",
    "        if self.root is None:\n",
    "            return \"Root is undefined\"\n",
    "        if hyperParams is None:\n",
    "            hyperParams = self.best\n",
    "        return self.root.predict(feature_vector, hyperParams)\n",
    "\n",
    "    def test(self, feature_vectors: pd.DataFrame, ground_truth: pd.DataFrame, hyperParams: int = None) -> float:\n",
    "        if self.root is None:\n",
    "            return \"Root is undefined\"\n",
    "        if hyperParams is None:\n",
    "            hyperParams = self.best\n",
    "        return self.root.test(feature_vectors, ground_truth, hyperParams)\n",
    "\n",
    "    \"\"\"Private Methods\"\"\"\n",
    "\n",
    "    def _makeFolds(self) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Select indices in each set.\n",
    "        \"\"\"\n",
    "        if self.y is None:\n",
    "            return ValueError(\"Data undefined\")\n",
    "        length: int = len(self.y)\n",
    "        order: np.ndarray = np.arange(0, length)\n",
    "        splits: list[np.ndarray] = np.array_split(order, 5)\n",
    "        self.splits = splits\n",
    "        return splits\n",
    "    \n",
    "    def _getParams(self):\n",
    "        \"\"\"Create list of hyperparameters to test\"\"\"\n",
    "        if len(self.X) != len(self.y) or len(self.X) == 0:\n",
    "            raise ValueError(\"Improper data\")\n",
    "        self.hyperParams = [i for i in range(2,len(self.X), 10)]\n",
    "        return self\n",
    "    \n",
    "    def _setup(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Set up self.validation to begin running\n",
    "        \"\"\"\n",
    "        if self.splits is None:\n",
    "            return ValueError(\"Data not yet split into folds\")\n",
    "        if self.y is None or self.X is None:\n",
    "            return ValueError(\"Data not yet defined\")\n",
    "        if not isinstance(self.hyperParams, list):\n",
    "            return TypeError(\"HyperParams needs to be of type list[int]\")\n",
    "        row_vector: np.ndarray = np.zeros((len(self.splits), len(self.y)), dtype=bool)\n",
    "        for i in range(len(self.splits)):\n",
    "            validation: np.ndarray = self.splits[i]\n",
    "            row_vector[i][validation] = True\n",
    "        row_vector = np.repeat(row_vector[np.newaxis, :, :], len(self.hyperParams), axis=0)\n",
    "        self.validation = row_vector\n",
    "        return row_vector\n",
    "    \n",
    "    def _findBest(self):\n",
    "        \"\"\"Compute errors of each HyperParameter\"\"\"\n",
    "        errors = np.zeros((self.validation.shape[0], self.validation.shape[1]), dtype=float)\n",
    "        validation = np.transpose(self.validation, (1, 0, 2))\n",
    "        # print(\"Validation Shape post tranposing: \", validation.shape)\n",
    "        # Shape should by (#folds, #hyperparams, 5197)\n",
    "        for i in range(validation.shape[0]): # For each fold\n",
    "            # print(\"Testing fold \", i+1)\n",
    "            # Rows are split by fold i.e. validation[i][j] = validation[i][k] since we expand self.validation by the number of folds we are measuring\n",
    "            node = RegressionTreeNode().train(self.X[~validation[i][0]], self.y[~validation[i][0]], 2)\n",
    "            for j in range(validation.shape[1]): # For each hyperParam\n",
    "                # print(\"HyperParam \", self.hyperParams[j])\n",
    "                # node = RegressionTreeNode().train(self.X[~validation[i][j]], self.y[~validation[i][j]], self.hyperParams[j])\n",
    "                error = node.test(self.X[validation[i][j]], self.y[validation[i][j]], self.hyperParams[j])\n",
    "                # print(\"Hyperparam: \", self.hyperParams[j], \" Error: \", error)\n",
    "                errors[j][i] = error\n",
    "\n",
    "        self.errors = errors\n",
    "        # errors is the MSE of each fold for each hyperparam\n",
    "        metrics = np.mean(errors, axis=-1) # metrics gives the MSE for each hyperparam\n",
    "        self.metrics = metrics\n",
    "        best = np.argmin(metrics)\n",
    "        self.best = self.hyperParams[best]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegressionTreeEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensemble class to boost Regression Tree\n",
    "Uses Forward Stagewise Additive Modelling\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class RegressionTreeEnsemble:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - trees: Ensemble of 5 RegressionTreeKFold\n",
    "    \n",
    "    Public Methods:\n",
    "        - train(X, y): Iteratively calculate the residuals between the ground truth and sum of all previous trees (up to 5)\n",
    "        - predict(X): Run predict on X over entire ensemble\n",
    "        - test(X, y): Predict all X and return MSE between prediction and ground truth y\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.trees: list[RegressionTreeKFold] = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        trees = []\n",
    "        residuals = y.to_numpy()\n",
    "\n",
    "        for i in range(1, 6):  # 5 trees\n",
    "            # print(f\"Training tree {i}\")\n",
    "            tree = RegressionTreeKFold().train(X, pd.DataFrame(residuals))\n",
    "            # print(f\"Best min_samples_split for tree {i}: {tree.best}\")\n",
    "            trees.append(tree)\n",
    "            cumulative_predictions = sum(X.apply(t.predict, axis=1).to_numpy() for t in trees)\n",
    "            residuals = y.to_numpy() - cumulative_predictions.reshape(-1, 1)\n",
    "\n",
    "        self.trees = trees\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return sum(X.apply(tree.predict, axis=1) for tree in self.trees)\n",
    "    \n",
    "    def test(self, X_test: pd.DataFrame, y_test:pd.DataFrame):\n",
    "        result = self.predict(X_test)\n",
    "        mse = (result.to_numpy()[:, np.newaxis] - y_test.to_numpy())**2\n",
    "        return np.mean(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Ensemble on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of custom ensemble on training data:  0.4019919200117535\n",
      "MSE of custom ensemble on test data:  0.5281653087508057\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n",
    "ensemble = RegressionTreeEnsemble().train(X, y)\n",
    "print(\"MSE of custom ensemble on training data: \", ensemble.test(X, y))\n",
    "print(\"MSE of custom ensemble on test data: \", ensemble.test(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of ensemble on training set:  0.40980966341572894\n",
      "Mean Squared Error of ensemble on test set:  0.5267987468000692\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Training here\n",
    "n_trees: int = 5\n",
    "predictions: np.ndarray = np.zeros(y.shape)\n",
    "models: list[DecisionTreeRegressor] = []\n",
    "\n",
    "for _ in range(n_trees):\n",
    "    residuals = y - predictions\n",
    "    # Define hyperparameters the same as are defined in RegressionTreeKFold: 2 to 5197 with intervals of 10\n",
    "    param_grid: dict = {'min_samples_split': [i for i in range(2, 5197, 10)]}\n",
    "    tree = DecisionTreeRegressor(random_state=1)\n",
    "    grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X, residuals)\n",
    "    best_tree = grid_search.best_estimator_\n",
    "    models.append(best_tree)\n",
    "    predictions += best_tree.predict(X).reshape(-1,1)\n",
    "\n",
    "# models is our ensemble of trees      \n",
    "\n",
    "training_predictions = np.zeros(y.shape)\n",
    "for model in models:\n",
    "    training_predictions += model.predict(X).reshape(-1,1)\n",
    "test_predictions = np.zeros(y_test.shape)\n",
    "for model in models:\n",
    "    test_predictions += model.predict(X_test).reshape(-1,1)\n",
    "\n",
    "mseTraining = mean_squared_error(y, training_predictions)\n",
    "mseTest = mean_squared_error(y_test, test_predictions)\n",
    "\n",
    "print(\"Mean Squared Error of ensemble on training set: \", mseTraining)\n",
    "print(\"Mean Squared Error of ensemble on test set: \", mseTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
